{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Generating refactored corpus\n",
    "Modify the synthetic benchmarks to generate a corpus of refactored programs.\n",
    "Generate 1 refactored program for each benchmark version."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import refactorings\n",
    "from pathlib import Path\n",
    "import difflib\n",
    "import random\n",
    "import shutil\n",
    "import subprocess\n",
    "import random"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Load projects\n",
    "Input data documented in `all.csv` and filtered in code."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Parse for ground truth.\n",
    "Ground truth means the flaw/fix locations (file/lineno) documented in the benchmark.\n",
    "Only parse it for buggy samples since that's all that's needed for refactoring."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "from collections import defaultdict\n",
    "import functools\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import re\n",
    "\n",
    "def corebench_manual():\n",
    "    result = defaultdict(functools.partial(defaultdict, list))\n",
    "    df = pd.read_csv('nb/corebench_manual_groundtruth.tsv', delimiter='\\t')\n",
    "    for _, row in df.iterrows():\n",
    "        for buggy_line in row[\"Line of Bug Crash\"].split(','):\n",
    "            buggy_line = int(buggy_line)\n",
    "            result[row[\"Project\"]][row[\"Buggy\"]].append((row[\"File of Bug\"], buggy_line))\n",
    "    # Convert to a normal dict\n",
    "    result = dict(result)\n",
    "    for k in result:\n",
    "        result[k] = dict(result[k])\n",
    "    return result\n",
    "\n",
    "def dbgbench(projects):\n",
    "    dbgbench = Path('dbgbench.github.io')\n",
    "    faultstxts = dbgbench.glob('*.faults.txt')\n",
    "    result = defaultdict(functools.partial(defaultdict, list))\n",
    "    for faultstxt in faultstxts:\n",
    "        project_name, ok = faultstxt.name.split('.')[:2]\n",
    "        if project_name == 'find':\n",
    "            project_name = 'findutils'\n",
    "        project = next((p for p in projects if p.program == project_name and p.ok == ok), None)\n",
    "        if project is not None:\n",
    "            with open(faultstxt) as f:\n",
    "                for l in f.readlines():\n",
    "                    filename, lineno = l.split(':')\n",
    "                    lineno = int(lineno)\n",
    "                    result[project.program][project.buggy].append((filename, lineno))\n",
    "    # Convert to a normal dict\n",
    "    result = dict(result)\n",
    "    for k in result:\n",
    "        result[k] = dict(result[k])\n",
    "    return result\n",
    "\n",
    "def synthetic(project):\n",
    "    flaws = set()\n",
    "    files = list(project.buggy_path.glob('*/*/*.c') + project.buggy_path.glob('*/*.c') + project.buggy_path.glob('*.c'))\n",
    "    assert len(files) > 0, project.buggy_path\n",
    "    for fname in files:\n",
    "        with open(fname) as f:\n",
    "            for i, line in enumerate(f.readlines(), start=1):\n",
    "                if project.program in 'abm':\n",
    "                    if re.search(r'/\\*\\s*BAD\\s*\\*/', line):\n",
    "                        flaws.add((fname, int(i)))\n",
    "                elif project.program == 'zitser':\n",
    "                    if re.search(r'/\\*\\s*BAD\\s*\\*/', line):\n",
    "                        flaws.add((fname, int(i+1)))\n",
    "                elif project.program == 'ctestsuite':\n",
    "                    if re.search(r'/\\*\\s*FLAW\\s*\\*/', line):\n",
    "                        flaws.add((fname, int(i)))\n",
    "                elif project.program == 'toyota':\n",
    "                    if re.search(r'/\\*\\s*ERROR', line):\n",
    "                        flaws.add((fname, int(i)))\n",
    "    return flaws\n",
    "\n",
    "def get_all_groundtruth(projects):\n",
    "    corebench_groundtruth = {}\n",
    "    corebench_groundtruth.update(corebench_manual())\n",
    "    corebench_groundtruth.update(dbgbench(projects))\n",
    "    for proj in projects:\n",
    "        if proj.program in corebench_groundtruth:\n",
    "            flaws = corebench_groundtruth[proj.program][proj.buggy]\n",
    "        else:\n",
    "            flaws = synthetic(proj)\n",
    "        # print(proj.program, proj.buggy, len(flaws))\n",
    "        proj.flaws = flaws\n",
    "    return projects\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Parse project info into neat dataclasses.\n",
    "Assumes all projects are in a folder named `tests`."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "import pandas as pd\n",
    "from dataclasses import dataclass, field\n",
    "from path import Path\n",
    "\n",
    "code_root = Path('tests')\n",
    "\n",
    "@dataclass(unsafe_hash=True)\n",
    "class ProjectInfo:\n",
    "    program: str = field(hash=True)\n",
    "    buggy: str = field(hash=True)\n",
    "    ok: str = field(hash=True)\n",
    "\n",
    "    def __post_init__(self):\n",
    "        if self.program == 'find':\n",
    "            self.program = 'findutils'\n",
    "        self.init_paths()\n",
    "        self.replace_unicode()\n",
    "    \n",
    "    @property\n",
    "    def versions(self):\n",
    "        return self.buggy, self.ok\n",
    "    \n",
    "    @property\n",
    "    def versions_with_paths(self):\n",
    "        return zip((self.buggy, self.ok), (self.buggy_path, self.ok_path))\n",
    "\n",
    "    def init_paths(self) -> str:\n",
    "        self.program_path = code_root / self.program\n",
    "        self.buggy_path = self.program_path / self.buggy\n",
    "        self.ok_path = self.program_path / self.buggy\n",
    "        if self.program in ['coreutils', 'findutils', 'grep', 'make']:\n",
    "            self.buggy_path = self.buggy_path / self.program\n",
    "            self.ok_path = self.ok_path / self.program\n",
    "        assert self.buggy_path.exists(), self.buggy_path\n",
    "        assert self.ok_path.exists(), self.ok_path\n",
    "\n",
    "    def replace_unicode(self):\n",
    "        files = list(self.buggy_path.glob('**/*.c') + self.buggy_path.glob('*.c') + self.ok_path.glob('**/*.c') + self.ok_path.glob('*.c'))\n",
    "        for fname in files:\n",
    "            with open(fname, 'r', encoding='utf-8', errors='replace') as f:\n",
    "                text = f.read()\n",
    "            with open(fname, 'w', encoding='utf-8') as f:\n",
    "                f.write(text)\n",
    "\n",
    "def get_iter(project_filter):\n",
    "    df = pd.read_csv('nb/all.csv')\n",
    "    \n",
    "    for _, row in df.iterrows():\n",
    "        if row[\"Program\"] in project_filter:\n",
    "            yield ProjectInfo(row[\"Program\"], row[\"Intro Commit ID\"], row[\"Fixed Commit ID\"])\n",
    "\n",
    "def get_projects(project_filter):\n",
    "    projects = list(get_iter(project_filter))\n",
    "    projects = get_all_groundtruth(projects)\n",
    "    projects = sorted(projects, key=lambda p: str(p.program_path))\n",
    "    return projects\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Filter project info"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "source": [
    "corebench_project_names = ['make', 'grep', 'coreutils', 'findutils']\n",
    "project_filter = []\n",
    "project_filter += ['abm']\n",
    "project_filter += ['ctestsuite']\n",
    "project_filter += ['zitser']\n",
    "project_filter += ['toyota']\n",
    "# project_filter += corebench_project_names\n",
    "projects = get_projects(project_filter)\n",
    "len(projects)"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "127"
      ]
     },
     "metadata": {},
     "execution_count": 8
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Refactor code\n",
    "\n",
    "Hyperparameters:\n",
    "- Random seed: should be reset before applying each refactoring.\n",
    "- `num_iterations`: Number of transformations to do\n",
    "- Which transforms to apply (see `refactoring`)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "source": [
    "from contextlib import redirect_stdout\n",
    "from tqdm import tqdm_notebook\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import re\n",
    "from shutil import ignore_patterns\n",
    "import os\n",
    "\n",
    "# Experiment parameters\n",
    "picker = refactorings.random_picker\n",
    "transforms = refactorings.all_refactorings\n",
    "factory = refactorings.TransformationsFactory(transforms, picker)\n",
    "\n",
    "# Zonk existing log files\n",
    "logfile = Path('nb/log.txt')\n",
    "if logfile.exists():\n",
    "    logfile.unlink()\n",
    "errors_log = Path('errors.log')\n",
    "if errors_log.exists():\n",
    "    errors_log.unlink()\n",
    "\n",
    "\n",
    "def get_exclude(bench_name, project, c_file):\n",
    "    \"\"\"Return the exclude pattern given a benchmark name, project directory and target C file.\"\"\"\n",
    "    srcfiles = ['*.c.diff', '*.c.back', '*.c.refactor', '*.c.transforms']\n",
    "    if bench_name == 'toyota':\n",
    "        for f in project.glob('src/*.c'):\n",
    "            if f.name == 'main.c' or f.name == c_file.name:\n",
    "                continue\n",
    "            else:\n",
    "                srcfiles.append(f.name)\n",
    "    return ignore_patterns(*srcfiles)\n",
    "\n",
    "\n",
    "def proj_and_files(proj):\n",
    "    result = set()\n",
    "    for f, _ in proj.flaws:\n",
    "        result.add((proj, f))\n",
    "    return list(result)\n",
    "\n",
    "\n",
    "print(f'Redirecting stdout to {logfile}')\n",
    "for project_name in sorted(list(set(p.program for p in projects))):\n",
    "    projects_and_files = []\n",
    "    for p in projects:\n",
    "        if p.program == project_name:\n",
    "            result = set()\n",
    "            for f, _ in p.flaws:\n",
    "                result.add((p, f))\n",
    "            projects_and_files.extend(result)\n",
    "    projects_and_files = sorted(projects_and_files, key=lambda p: str(p[1]))\n",
    "    for proj, c_file in tqdm_notebook(projects_and_files, desc=project_name):\n",
    "        avoid = []\n",
    "        for f, lineno in proj.flaws:\n",
    "            if f == c_file:\n",
    "                avoid.append(lineno)\n",
    "        assert len(avoid) > 0\n",
    "        random.seed(0)\n",
    "        with open(logfile, 'a') as f:\n",
    "            with redirect_stdout(f):\n",
    "                exclude = get_exclude(proj.program, proj.buggy_path, c_file)\n",
    "                with factory.make_project(c_file, proj.buggy_path, exclude, avoid=avoid) as refactoring_project:\n",
    "                    print('***REFACTORING***', refactoring_project, c_file)\n",
    "                    tmp_c_file, transforms_applied = refactoring_project.apply_all(return_applied=True)\n",
    "                    new_c_file = Path(str(c_file) + '.refactor')\n",
    "                    shutil.copy(tmp_c_file, new_c_file)\n",
    "                    with open(c_file.parent / (c_file.name + '.transforms'), 'w') as f:\n",
    "                        f.write('\\n'.join(t.__name__ for t in transforms_applied))\n",
    "                    with open(c_file) as old_f, open(new_c_file) as new_f:\n",
    "                        diff = list(difflib.unified_diff(old_f.readlines(), new_f.readlines(), fromfile=c_file.name, tofile=c_file.name))\n",
    "                    with open(str(c_file) + '.diff', 'w') as f:\n",
    "                        f.write(''.join(diff))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Redirecting stdout to log.txt\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='toyota', max=46, style=ProgressStyle(description_width='initiâ€¦"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "34f945a8bda149b79ad449883c165565"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "source": [
    "import pandas as pd\n",
    "import difflib\n",
    "\n",
    "all_transform_names = sorted([t.__name__ for t in refactorings.all_refactorings])\n",
    "\n",
    "with open('nb/stats.csv', 'w') as csv_f:\n",
    "    print(','.join(('project', 'version', *all_transform_names, 'lines changed')), file=csv_f)\n",
    "    for proj in projects:\n",
    "        # Get files and check they exist\n",
    "        try:\n",
    "            transforms_file = next(proj.buggy_path.walk('*.c.transforms'))\n",
    "            new_file = transforms_file.with_suffix('.refactor')\n",
    "            old_file = new_file.parent / new_file.stem\n",
    "        except:\n",
    "            raise Exception(f'{proj.program}-{proj.buggy}')\n",
    "        assert transforms_file.exists()\n",
    "        assert new_file.exists(), new_file\n",
    "        assert old_file.exists(), old_file\n",
    "\n",
    "        # Collect which transforms were applied\n",
    "        with open(transforms_file) as f:\n",
    "            transforms_applied = set(f.read().splitlines())\n",
    "        was_applied = [t in transforms_applied for t in all_transform_names]\n",
    "\n",
    "        # Collect number of changed lines\n",
    "        differences = sum(1 for d in difflib.ndiff(old_file.open().readlines(), new_file.open().readlines()) if d[0] in ('+', '-'))\n",
    "\n",
    "        print(','.join((proj.program, proj.buggy, *('TRUE' if a else 'FALSE' for a in was_applied), str(differences))), file=csv_f)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "source": [
    "import subprocess\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import tqdm.notebook as tqdm\n",
    "import shutil\n",
    "import re\n",
    "\n",
    "exclude = []\n",
    "failed = []\n",
    "pbar = tqdm.tqdm(projects)\n",
    "for proj in pbar:\n",
    "    project = proj.program\n",
    "    code_dir = proj.buggy_path\n",
    "    assert code_dir.exists()\n",
    "\n",
    "    # Replace refactored file\n",
    "    proj_files = {f for f, _ in proj.flaws}\n",
    "    proj_files = list(sorted(proj_files))\n",
    "    for original in proj_files:\n",
    "        pbar.set_postfix({'excluded': len(exclude), 'failed': len(failed), 'file': original})\n",
    "        refactored = original.with_suffix('.c.refactor')\n",
    "        backup = refactored.with_suffix('.back')\n",
    "        assert refactored.exists(), refactored\n",
    "        assert original.exists(), original\n",
    "        if not backup.exists():\n",
    "            shutil.copy2(original, backup)\n",
    "        assert backup.exists(), backup\n",
    "        \n",
    "        # Fix up the makefile which might reference the *-ok/*-bad filenames\n",
    "        makefile = refactored.parent / 'Makefile'\n",
    "        assert makefile.exists()\n",
    "        makefile_backup = refactored.parent / 'Makefile.okbad'\n",
    "        if makefile_backup.exists():\n",
    "            shutil.copy2(makefile_backup, makefile)\n",
    "        else:\n",
    "            shutil.copy2(makefile, makefile_backup)\n",
    "        with open(makefile) as f:\n",
    "            text = f.read()\n",
    "        text = re.sub(r'(\\w+)-ok', r'\\1', text)\n",
    "        text = re.sub(r'(\\w+)-bad', r'\\1', text)\n",
    "        if (proj.program, proj.buggy) in (('ctestsuite', '095'), ('ctestsuite', '097'), ('ctestsuite', '099'), ('ctestsuite', '187')):\n",
    "            text = text.replace('$(CC) $(CFLAGS) $(SRC)', '$(CC) $(CFLAGS) $(SRC) $(shell mysql_config --cflags --libs)')\n",
    "        if (proj.program, proj.buggy) in (('zitser', '287'), ('zitser', '289'), ('zitser', '295')):\n",
    "            text = text.replace('create\\n', './create\\n')\n",
    "        with open(makefile, 'w') as f:\n",
    "            f.write(text)\n",
    "\n",
    "        # Build and check\n",
    "        proc = subprocess.run('make clean', cwd=str(code_dir), capture_output=True, shell=True)\n",
    "        proc = subprocess.run('make', cwd=str(code_dir), capture_output=True, shell=True)\n",
    "        if proc.returncode != 0:\n",
    "            exclude.append((project, proj.buggy, proc.stderr.decode()))\n",
    "            continue\n",
    "        \n",
    "        try:\n",
    "            shutil.copy(refactored, original)\n",
    "            proc = subprocess.run('make clean', cwd=str(code_dir), capture_output=True, shell=True)\n",
    "            proc = subprocess.run('make', cwd=str(code_dir), capture_output=True, shell=True)\n",
    "            if proc.returncode != 0:\n",
    "                failed.append((project, proj.buggy, proc.stderr.decode()))\n",
    "        finally:\n",
    "            shutil.copy(backup, original)\n",
    "\n",
    "# install mysql-devel and pam-devel first.\n",
    "# Some projects in ctestsuite won't build.\n",
    "print(len(exclude), 'versions didn\\'t build')\n",
    "with open('nb/exclude.csv', 'w') as f:\n",
    "    f.write(f'project,version\\n')\n",
    "    for project, version, e in exclude:\n",
    "        f.write(f'{project},{version}\\n')\n",
    "with open('nb/exclude.log', 'w') as f:\n",
    "    for project, version, e in exclude:\n",
    "        f.write(f'***tests/{project}/{version}***\\n')\n",
    "        f.write(f'{e}\\n')\n",
    "\n",
    "print(len(failed), 'versions failed after refactoring')\n",
    "with open('nb/failed.csv', 'w') as f:\n",
    "    f.write(f'project,version\\n')\n",
    "    for project, version, e in failed:\n",
    "        f.write(f'{project},{version}\\n')\n",
    "with open('nb/failed.log', 'w') as f:\n",
    "    for project, version, e in failed:\n",
    "        f.write(f'***tests/{project}/{version}***\\n')\n",
    "        f.write(f'{e}\\n')"
   ],
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=1), HTML(value='')))"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "0b8ee09f98b64b27abbebbc1ec5aa3bb"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n",
      "0 versions didn't build\n",
      "1 versions failed after refactoring\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "To pack up refactorings, use `tar cf refactors.tar $(find itc -name '*.c.reformat' -o -name '*.c.diff' -o -name '*.transforms.txt')`."
   ],
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3",
   "language": "python"
  },
  "interpreter": {
   "hash": "3fc56e3fab8c747237b055ef0e7d2321d1637d9af88e6fee51f2ecb6862cb686"
  },
  "language_info": {
   "name": "python",
   "version": "3.7.4",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}